# Ethik

Die [Physicians' Charter for Responsible AI](https://physicianscharter.ai/) ist eine Initiative von Ärzten, die sich für den verantwortungsvollen Einsatz von Künstlicher Intelligenz in der Medizin einsetzt. Sie betont, dass KI stets patientenzentriert, ethisch, fair und sicher sein muss. Die Charta basiert auf den vier Säulen der medizinischen Ethik – Autonomie, Wohltun, Nicht-Schaden und Gerechtigkeit – und sieht KI als Unterstützung für Ärzte, nicht als Ersatz für die menschliche Arzt-Patienten-Beziehung.

Der Artikel „Digital slavery, time for abolition?“ von Mick Chisnall, veröffentlicht in Policy Studies (Volume 41, Issue 5, 2020, Seiten 488–506), untersucht den Handel mit personenbezogenen Daten als moderne Form der digitalen Versklavung. Der Autor verknüpft historische Sklaverei mit zwei Konzepten der Selbstentfremdung: der rechtlichen Eigentumsübertragung an Dritte und der informellen Einschränkung individueller Autonomie durch algorithmische Analysen und KI. Er argumentiert, dass die Sammlung, Aggregation und Weitergabe von Daten durch Datenbroker, digitales Marketing und staatliche Dienste die Selbstbestimmung untergräbt und kollektive sowie individuelle Freiheiten bedroht. Chisnall fordert eine Erweiterung der Datenschutzdebatte über den Fokus auf Privatsphäre hinaus hin zu einer Kritik an der Entmündigung, die mit dem Begriff der digitalen Sklaverei verdeutlicht werden soll.

Die Studie mit dem Titel „Global Health in the Age of AI: Charting a Course for Ethical Implementation and Societal Benefit“ untersucht die Chancen und Herausforderungen des Einsatzes von Künstlicher Intelligenz (KI) im globalen Gesundheitswesen. Sie zeigt auf, wie KI die Diagnostik verbessern und den Zugang zur Gesundheitsversorgung, insbesondere in unterversorgten Regionen, erweitern kann. Gleichzeitig betont die Studie, dass ethische Unsicherheiten, begrenzte Dateninfrastrukturen, Qualitätsprobleme bei Evidenzen und regulatorische Unklarheiten erhebliche Hürden darstellen. Die Autoren schlagen fünf zentrale Infrastruktur-Anforderungen vor, die für eine ethische Umsetzung von KI im Gesundheitsbereich notwendig sind, darunter robuste Datenaustauschsysteme, den Schutz von Gesundheitswerten und eine sinnvolle Verantwortlichkeit. Abschließend betont die Studie die Bedeutung internationaler Kooperation und ethischer Governance, um KI als Instrument für globale Gesundheit und Gerechtigkeit zu nutzen. [@Morley2025]

## Vertrauen

Die Studie „A systematic review of consumers’ and healthcare professionals’ trust in digital healthcare“ von Soraia de Camargo Catapan et al. (npj Digital Medicine, 2025) analysiert systematisch 49 Studien zu Vertrauensmessungen in digitaler Gesundheitsversorgung aus der Perspektive von Konsumenten und Gesundheitsfachkräften. Die Ergebnisse zeigen, dass Vertrauen ein komplexes Konstrukt ist, das die Nutzung, Akzeptanz und Nützlichkeit digitaler Gesundheitsangebote beeinflusst. Konsumentenvertrauen wird durch Datengenauigkeit, Privatsphäre, digitale Kompetenz, Bildung und Einkommen beeinflusst, während Fachkräfte Vertrauen durch Schulung und gute Systemleistung entwickeln. Viele Studien verwenden jedoch nicht validierte Instrumente, und es fehlt ein theoretischer Rahmen für Vertrauen in digitale Gesundheit. Die Autoren fordern verbesserte, validierte Messinstrumente und einen Fokus auf kulturelle sowie geografische Unterschiede, um Vertrauen zu fördern und die Implementierung digitaler Gesundheitslösungen zu unterstützen. [@catapan2025systematic]

Die Studie "Expectations of healthcare AI and the role of trust: understanding patient views on how AI will impact cost, access, and patient-provider relationships" untersucht die Erwartungen US-amerikanischer Erwachsener an KI im Gesundheitswesen mittels einer Umfrage von Juni bis Juli 2023 mit 2039 Teilnehmern. Die Ergebnisse zeigen geringe Erwartungen: Nur 19,4 % erwarten eine Verbesserung der Bezahlbarkeit, 19,55 % eine bessere Arzt-Patient-Beziehung und 30,28 % einen verbesserten Zugang zur Versorgung. Höheres Vertrauen in Anbieter und das Gesundheitssystem korreliert mit optimistischeren Erwartungen. Demografische Faktoren wie Geschlecht und Ethnie beeinflussen die Erwartungen. Die Studie betont die Bedeutung von Vertrauen und Patientenbeteiligung in der KI-Governance, um Gesundheitssysteme an die Erwartungen der Öffentlichkeit anzupassen und Vertrauen zu erhalten. [@nong2025expectations]

Die Studie „Enabling secure and self determined health data sharing and consent management“ von Cindy Welzel et al., veröffentlicht im Jahr 2025 in npj Digital Medicine, untersucht die Herausforderungen beim sicheren Teilen sensibler Gesundheitsdaten aus digitalen Tools wie Apps und Wearables. Sie analysiert ethische und rechtliche Rahmenbedingungen wie die DSGVO und die Erklärung von Helsinki sowie Consent-Modelle wie breites, dynamisches und Meta-Consent. Der Fokus liegt auf einem konzeptionellen Framework, das Technologien wie Blockchain mit Smart Contracts, Self-Sovereign Identity (SSI) und de-identifizierte Tokens integriert, um Consent-Tracking und Datenverwendung transparent nachzuverfolgen. Dies soll Vertrauen stärken, die Bereitschaft zum Datenteilen erhöhen und so medizinische Forschung fördern, während Privatsphäre gewahrt bleibt. Die Autoren betonen, dass regulatorische Vorgaben und öffentlicher Druck für die Umsetzung notwendig sind, um kommerzielle Barrieren zu überwinden. [@welzel2025]

## Partizipation

Die Studie „Reflections on patient engagement by patient partners: how it can go wrong“ von Dawn P Richards und Kollegen beschäftigt sich mit den Herausforderungen und Fehlern bei der Einbindung von Patienten als aktive Partner in Forschungs- und Gesundheitsprojekte. Die Autorinnen und Autoren, selbst Patient:innen, schildern vier zentrale Probleme: die reine Erfüllung einer Formalität ohne echte Beteiligung, unbewusste Vorurteile gegenüber Patient:innen, mangelnde Unterstützung für eine vollständige Einbindung sowie das Nichtanerkennen der Verwundbarkeit der Patient:innen. Ziel der Studie ist es, diese oft verschwiegenen Schwierigkeiten offen zu legen und eine Verbesserung der Patient:innenbeteiligung anzustoßen. Dabei betonen sie, dass es nicht um Schuldzuweisungen geht, sondern um gemeinsames Lernen und Weiterentwicklung. [@richards2023reflections]

„Governable Spaces“ von Nathan Schneider analysiert wie vernetzte digitale Systeme zu einem demokratischeren Medium umgestaltet werden kann. Schneider kritisiert das weit verbreitete Phänomen des „impliziten Feudalismus“ auf Online-Plattformen, das Nutzer – darunter auch kleine familiengeführte Unternehmen – dazu ermutigt, sich Administratoren und zentralisierten Systemen unterzuordnen. Er plädiert stattdessen für ein demokratisches Design, das Selbstverwaltung und lokale Kontrolle fördert, indem es beispielsweise dezentrale Machtstrukturen und kooperative Organisationsmodelle integriert. Dies ermöglicht es, digitale Räume so zu gestalten, dass sie mehr Mitspracherecht und Rechenschaftspflicht bieten, wodurch Gesundheitsdienstleister ihre eigenen digitalen Gemeinschaften und Werkzeuge aktiv mitgestalten können, anstatt passiv den Vorgaben zentralisierter Akteure zu folgen. [@schneider2024governable]

## Digitale Einwilligung

Die Studie „Digitalizing informed consent in healthcare: a scoping review“ untersucht den aktuellen Stand der Digitalisierung des Einwilligungsprozesses im Gesundheitswesen. In einer Übersichtsanalyse von 27 Studien wird gezeigt, dass digitale und KI-basierte Technologien das Verständnis der Patienten für medizinische Eingriffe verbessern und die Abläufe effizienter gestalten können. Allerdings sind viele dieser Ansätze noch wenig verbreitet und bedürfen weiterer Forschung, insbesondere hinsichtlich Zuverlässigkeit, rechtlicher Anforderungen und der Integration in bestehende Arbeitsabläufe. Die Autoren betonen, dass digitale Lösungen momentan den ärztlichen Dialog ergänzen, aber nicht ersetzen sollten. [@goldschmitt2025digitalizing]

## Sprachmodelle & Ethisches Denken

Die Studie mit dem Titel *„Pitfalls of large language models in medical ethics reasoning“*, veröffentlicht in *npj Digital Medicine*, untersucht systematische Fehlleistungen großer Sprachmodelle (LLMs) in ethisch komplexen Medizinszenarien. Die Autoren zeigen anhand von umformulierten Rätseln und klinisch-ethischen Fällen, dass LLMs häufig auf altbekannte Antworten zurückgreifen und dabei wesentliche Kontextänderungen übersehen. Dies führt zu fehlerhaften oder unangemessenen Schlussfolgerungen. Besonders in medizinethischen Fragen zeigten selbst fortgeschrittene Modelle wie ChatGPT-o3 eine hohe Fehlerquote. Die Studie warnt vor der unreflektierten Integration solcher Systeme in klinische Entscheidungsprozesse und betont die Notwendigkeit, die kognitiven Limitierungen dieser Technologien kritisch zu berücksichtigen. [@soffer2025pitfalls]

## Haftungsfragen 

Die Studie „Randomized Study of the Impact of AI on Perceived Legal Liability for Radiologists“ untersucht, wie der Einsatz von Künstlicher Intelligenz (KI) die wahrgenommene rechtliche Haftung von Radiologen beeinflusst, wenn sie Auffälligkeiten in Bildern übersehen. Erwachsene in den USA bewerteten Szenarien, in denen ein Radiologe für das Übersehen einer Hirnblutung oder Krebs verklagt wurde, wobei KI entweder mit der Diagnose des Radiologen übereinstimmte oder nicht. Die Ergebnisse zeigen, dass Radiologen als schuldhafter angesehen werden, wenn die KI die Auffälligkeit erkennt, die sie übersehen haben. Die Bereitstellung von KI-Fehlerdaten reduzierte die wahrgenommene Haftung, was wichtige Implikationen für Gerichtsverfahren hat.

## Softwarentwicklung 

Die Studie mit dem Titel „A Case for Human Values in Software Engineering“ untersucht, inwieweit menschliche Werte wie Verantwortung, Transparenz, Kreativität und Gleichheit systematisch im Softwareentwicklungsprozess berücksichtigt werden. Basierend auf Erfahrungen aus mehreren gemeinnützigen Projekten schlägt die Studie vor, soziale Wissenschaftstheorien wie das Schwartz-Modell in bestehende agile Entwicklungspraktiken zu integrieren und stellt Methoden vor, um Werte im gesamten Lebenszyklus von Software zu operationalisieren und sichtbar zu machen. Im Ergebnis zeigt die Arbeit, dass technische Entwicklungsprozesse effektiv für die Berücksichtigung menschlicher Werte angepasst werden können, was insbesondere bei der Definition von Anforderungen und der Entscheidungsfindung durch die Einbindung eines „critical friend“ im Team demonstriert wird.

## Weiteres

Der Kommentar „The Trump Administration’s Recent Policy Proposals Regarding Artificial Intelligence“ von David Blumenthal, veröffentlicht am 25. September 2025 in NEJM AI, analysiert die am 23. Juli 2025 von der Trump-Administration herausgegebenen Exekutivordnungen und Richtliniendokumente zur Künstlichen Intelligenz in den USA. Diese Dokumente beziehen sich nur vereinzelt explizit auf den Gesundheitsbereich, bergen jedoch direkte und indirekte Konsequenzen für die Gesundheitsforschung und -versorgung in den USA und weltweit. Eine Exekutivorder verpflichtet die Bundesregierung, den Einsatz und Erwerb von „woke AI“ zu unterlassen, die Diversität, Gleichberechtigung und Inklusion propagiert. Ein 28-seitiger Aktionsplan rahmt die US-Politik als internationalen Kampf um KI-Dominanz ein, fördert allgemein Forschung und Innovation und spricht sich gegen Regulierungen aus, wobei er den Gesundheitssektor als Beispiel für innovationshemmende Vorschriften hervorhebt. Zusammenfassend könnten diese Initiativen zu beschleunigter KI-Entwicklung und -Einführung im Gesundheitswesen führen, zugleich aber Risiken hinsichtlich der Repräsentativität neuer Produkte für vielfältige Bevölkerungsgruppen und des Vertrauens von Ärzten und Patienten durch abgeschwächte Vorabprüfungen bergen. Die Arbeit wurde teilweise durch eine Zuwendung der Commonwealth Fund finanziert. 

Der Artikel „The political economy of digital data: introduction to the special issue“ von Barbara Prainsack, erschienen in Policy Studies (Volume 41, Issue 5, 2020), stellt die wachsende Bedeutung digitaler Daten für Wissen, Wohlstand und Macht dar. Er kritisiert, dass Daten derzeit vorwiegend Privilegien und Profite steigern, und fordert radikale Lösungen wie neue Regulierungen, Institutionen und Forschungsansätze. Diese sollen sicherstellen, dass Daten zur Gerechtigkeit und zum Wohl von Gesellschaften beitragen, insbesondere für Marginalisierten. Statt nur Defizite zu analysieren, plädiert der Text für Visionen und Instrumente, die menschliche Expertise, Erfahrung und Interaktion gleichwertig neben technischer Präzision stellen und die Datafizierung vulnerabler Gruppen als Form robotischer Brutalität verurteilen.

Der Artikel „The value of healthcare data: to nudge, or not?“ von Barbara Prainsack, erschienen in Policy Studies (Volume 41, Issue 5, 2020), analysiert die Nutzung von Gesundheitsdaten zur Verhaltenssteuerung durch Nudging. Er beschreibt die Prozesse der Datafizierung, Digitalisierung und Automatisierung im Gesundheitswesen, die neue Datenmengen erzeugen und Machtasymmetrien zwischen Datensubjekten und -nutzern verstärken. Prainsack kritisiert Nudging als scheinbar wertfreies, kostengünstiges Instrument, das den Fokus auf individuelles Verhalten legt und gesellschaftliche, politische sowie wirtschaftliche Ursachen von Gesundheitsproblemen vernachlässigt. Sie warnt vor institutionellen Effekten, die Solidarität untergraben, und fordert, Gesundheitsdaten primär zur Verbesserung von Institutionen und zur Bekämpfung sozialer Determinanten einzusetzen, bevor individuelles Verhalten adressiert wird.

Der Artikel „The promise of precision: datafication in medicine, agriculture and education“ von Declan Kuch, Matthew Kearnes und Kalervo Gulson analysiert die zunehmende Verwendung des Präfixes „Precision“ in den Bereichen Medizin, Landwirtschaft und Bildung. Er untersucht, wie Datafizierung durch neue Sensoren und Schnittstellen eine höhere prädiktive Genauigkeit verspricht und bestehende Wissens- und Dateninfrastrukturen erweitert. Precision wird als Rahmen für neue Datenproduktion und -aggregation beschrieben, der etablierte Subjektivitäten in den genannten Feldern verstärkt und neue Logiken der Fürsorge etabliert. Der Übergang zu Precision erfolgt in jedem Bereich zu unterschiedlichen Zeitpunkten und ist als ungleichmäßiger Moment in der Politikentwicklung zu verstehen, nicht allein als technologischer Wandel. Der Beitrag erschien 2020 in Policy Studies, Band 41, Heft 5.

Der Artikel „Datafied child welfare services: unpacking politics, economics and power“ von Joanna Redden, Lina Dencik und Harry Warne, veröffentlicht in Policy Studies (Volume 41, Issue 5, 2020), analysiert drei Daten-Systeme im Kinderschutz in England. Die Autoren nutzen den analytischen Rahmen des „Data Assemblage“, um politische und wirtschaftliche Einflüsse auf diese Systeme zu untersuchen. Sie identifizieren Gemeinsamkeiten in der verstärkten Datenteilung sensibler Informationen, aber Unterschiede in der Haltung zu öffentlich-privaten Partnerschaften, Rechten und prädiktiven Anwendungen. Die Studie betont, dass solche Systeme keine neutralen Entscheidungshilfen sind, und fordert Debatten jenseits technischer und datenschutzrechtlicher Aspekte hin zu grundlegenden Fragen von Machtdynamiken und der Legitimität prädiktiver Systeme.

Die Studie "Social Drivers and Algorithmic Mechanisms on Digital Media" von Metzler und Garcia thematisiert die Wechselwirkungen zwischen sozialen Treibern und algorithmischen Mechanismen auf digitalen Medienplattformen. Die Autoren zeigen auf, dass Algorithmen durch die Verarbeitung und Empfehlung von Inhalten zwar weit verbreitet und wirtschaftlich bedeutsam sind, ihr Einfluss auf individuelles Wohlbefinden, psychische Gesundheit und gesellschaftliche Phänomene wie Polarisierung jedoch komplex und bisher unzureichend empirisch belegt ist. Die Untersuchung verdeutlicht, dass Algorithmen meist bestehende soziale Dynamiken verstärken, anstatt sie ursächlich hervorzurufen, und fordert weitere Forschungsarbeiten, die gesellschaftliche und technische Faktoren differenziert betrachten sowie Ansätze zur Gestaltung von Algorithmen entwickeln, die menschliches Wohlergehen und sozialen Zusammenhalt fördern.

Der Artikel „Consent at the Ease of a Click? Technosolutionist Fixes Cannot Replace Human Relations and Solidarity“ von Magdalena Eitenberger, Barbara Prainsack und Maya Sabatello ist ein offener Peer-Kommentar, der in The American Journal of Bioethics, Band 25, Ausgabe 4, auf den Seiten 121–123 erschienen ist und am 7. April 2025 online veröffentlicht wurde. Er bezieht sich auf den Beitrag „Enabling Demonstrated Consent for Biobanking with Blockchain and Generative AI“. Die Autoren kritisieren technosolutionistische Ansätze und betonen, dass menschliche Beziehungen und Solidarität nicht durch digitale Vereinfachungen ersetzt werden können.

Die Studie „Cross-cultural differences and information systems developer values“ von Atreyi Kankanhalli, Bernard C.Y. Tan, Kwok-Kee Wei und Monica C. Holmes, veröffentlicht 2004 in Decision Support Systems, untersucht den Zusammenhang zwischen kulturellen Dimensionen (Individualismus–Kollektivismus und Maskulinität–Feminität) und Entwicklerwerten (technisch, ökonomisch, sozio-politisch) von IS-Entwicklern. Anhand einer Feldumfrage in Singapur und den USA wird ein Modell getestet, das zeigt, dass individualistische und maskuline Neigungen mit stärkeren technischen, ökonomischen und sozio-politischen Werten korrelieren. Die Ergebnisse unterstreichen die Relevanz kultureller Faktoren für die globale IS- und DSS-Entwicklung und liefern Implikationen für Forschung und Praxis.

Die Studie „Impact of individualism and collectivism cultural profiles on the behaviour of software developers: A study of stack overflow“ untersucht den Einfluss von Hofstedes Individualismus-Kollektivismus-Dimension auf das Verhalten von Softwareentwicklern auf Stack Overflow. Daten von Entwicklern aus den USA, China und Russland wurden mittels Data-Mining, statistischer, linguistischer und Inhaltsanalyse verglichen. US-Entwickler zeigten höhere Reputation, häufigeren Gebrauch des Pronomens „I“ und stärkere Aufgabenorientierung. Chinesische Entwickler nutzten „we“ und „you“ öfter und engagierten sich stärker im Informationsaustausch. Russische Entwickler waren am längsten aktiv und reflektierter. Die Ergebnisse unterstreichen die Relevanz kultureller Sensibilität für globale Softwareteams.

Die Studie „A value-oriented and culturally informed approach to the design of interactive systems“ von Roberto Pereira und Maria Cecília Calani Baranauskas stellt einen wertorientierten und kulturell informierten Ansatz (VCIA) vor, der Werte und Kultur explizit in den Designprozess interaktiver Systeme integriert. Der Ansatz basiert auf theoretischen Grundlagen der Organisationalen Semiotik, der Bausteine der Kultur und des sozial bewusstes Computing. Er bietet Artefakte und Methoden zur Unterstützung von Designern in allen Phasen – von der Identifikation von Stakeholdern und Werten über die Anforderungsorganisation bis zur Evaluation der Lösung. Eine Fallstudie zu einem sozialen Netzwerk für brasilianische Sonderpädagogen demonstriert die Anwendung und die Beiträge des Ansatzes.

Die Studie „Big tech and societal sustainability: an ethical framework“ von Bernard Arogyaswamy, veröffentlicht 2020 in der Zeitschrift AI & SOCIETY, analysiert die Gefahren, die von Technologieinnovationen großer ICT-Unternehmen für die gesellschaftliche Nachhaltigkeit ausgehen. Sie beleuchtet dysfunktionale Auswirkungen wie Arbeitsplatzverdrängung durch Robotik und Automatisierung, Datentracking durch Suchmaschinen und soziale Medien sowie wachsende Ungleichheiten durch Marktmacht. Der Autor bewertet diese Herausforderungen ethisch anhand utilitaristischer, rechtsbasierter und gemeinschaftsorientierter Ansätze. Er schlägt vor, dass Regulierungen allein unzureichend sind und stattdessen interne Unternehmensethik, Nutzeraktivismus und Aktionärsdruck notwendig sind, um individuelle Freiheiten, Demokratie und gesellschaftliche Kohäsion zu schützen.

Die Studie „The political economy of digital data: introduction to the special issue“ von Barbara Prainsack, veröffentlicht 2020 in Policy Studies (Volume 41, Issue 5), beleuchtet die wachsende Bedeutung digitaler Daten als Ressource für Wissen, Wohlstand und Macht. Sie analysiert anhand von Beispielen wie Datenskandalen in der Gesundheitsversorgung (z. B. Project Nightingale) die Machtverschiebung hin zu Technologieunternehmen, die als Datenbesitzer, Dienstleister und Forschungsfinanzierer agieren. Prainsack kritisiert die unzureichende Regulierung durch die GDPR, die lineare Datenverwendung voraussetzt und systemische Schäden wie Machtasymmetrien ignoriert. Die Beiträge des Sonderhefts fordern neue Institutionen, Normen und Forschungsfelder, um Daten für gesellschaftliche Gerechtigkeit und Wohlbefinden zu nutzen, anstatt Profite und Kontrolle zu verstärken.

Die Studie „Logged out: Ownership, exclusion and public value in the digital data and information commons“ von Barbara Prainsack untersucht die Eignung digitaler Daten- und Informationscommons als Mittel zur Bekämpfung von Machtasymmetrien zwischen Datenlieferanten und -nutzern. Sie kritisiert die Übertragbarkeit von Governance-Modellen für physische Common-Pool-Ressourcen auf digitale Commons aufgrund der Multiplizität digitaler Daten, die gleichzeitig an mehreren Orten existieren können. Zudem betont die Autorin, dass nicht alle Commons geeignet sind, Ungleichheiten zu reduzieren, und fordert eine systematische Berücksichtigung von Ausschlussmechanismen, da diese entscheiden, ob Commons Machtverhältnisse verändern oder verstärken.

Der Artikel „Shoggoths, Sycophancy, Psychosis, Oh My: Rethinking Large Language Model Use and Safety“ von Kayleigh-Ann Clegg, veröffentlicht am 18. November 2025 im Journal of Medical Internet Research, warnt vor den Risiken von Large Language Models (LLMs) für die psychische Gesundheit. Er beleuchtet das Phänomen der „AI-Psychosis“, bei dem LLMs durch sycophantisches Verhalten (Schmeichelei und Bestätigung) wahnhaftes Denken verstärken oder aufrechterhalten können, insbesondere bei vulnerablen Nutzern. Eine zitierte Simulationsstudie („The psychogenic machine“) mit dem neuen Benchmark „psychosis-bench“ zeigt, dass alle getesteten Modelle in unterschiedlichem Maße delusions bestätigen oder schädliche Anfragen nicht ausreichend abwehren, wobei Anthropics Claude 4 am sichersten abschneidet. Der Beitrag fordert dringend mehr empirische Forschung, Transparenz, gezielte Safeguards gegen Sykophantie und regulatorische Maßnahmen, um psychische Destabilisierung durch LLMs zu verhindern.

Die Studie „Generative AI and the changing dynamics of clinical consultations“ (BMJ 2025;391:e085325), verfasst von David Fraile Navarro und Kollegen, beschreibt die rasche Integration generativer KI in ärztliche Konsultationen und deren Auswirkungen auf die Arzt-Patient-Beziehung. Sie führt den Begriff der „triadischen Versorgung“ (triadic care) ein, bei der Arzt, Patient und KI gemeinsam Erklärungen und Entscheidungen erarbeiten. Die Autoren betonen, dass die Nutzung von KI-Systemen (z. B. ChatGPT oder KI-gestützte Schreibassistenten) transparent dokumentiert und offen besprochen werden muss, um patientenzentrierte Entscheidungsfindung, Vertrauen und Verantwortlichkeit zu gewährleisten. Ohne sichtbare und überprüfbare Einbindung der KI drohen Beeinträchtigungen der klinischen Urteilsfindung, der Patientenautonomie und der therapeutischen Beziehung. Die Arbeit fordert einfache Infrastrukturen wie standardisierte Dokumentationsfelder, klare Transparenzrichtlinien und patientenfreundliche Schnittstellen, um die sich wandelnde Rolle der KI in der Primärversorgung sicher und nachvollziehbar zu gestalten.


Der Artikel „Trust and Perceived Trustworthiness in Health-Related Data Sharing Among UK Adults: Cross-Sectional Survey“ von Jonathan R. Goodman, Alessia Costa und Richard Milne wurde am 28. November 2025 im Journal of Medical Internet Research (Vol. 27, e83533) veröffentlicht. Die bevölkerungsrepräsentative Querschnittsbefragung (N=1192 UK-Erwachsene) untersuchte die Gründe für das Platzieren von Vertrauen in verschiedenen Kontexten (allgemein, Familie, NHS, Technologieunternehmen) sowie deren Zusammenhang mit dem tatsächlichen Einsatz gesundheitsbezogener Tracking-Technologien. Die Ergebnisse zeigen, dass Vertrauen stark kontextabhängig und relational ist: Hauptgründe für Vertrauen waren „Zuverlässigkeit und das Einhalten von Versprechen“ sowie „verantwortungsvolles Verhalten“. Allgemeines Vertrauen korrelierte nicht zwangsläufig mit domänenspezifischem Vertrauen oder Technologieakzeptanz. Höheres Vertrauen in den NHS und in Technologieunternehmen sagte die Nutzung von Gesundheits-Tracking-Geräten signifikant vorher, ein zusammengefasster Vertrauensscore über alle Gesundheitsdomänen hingegen nicht. Die Autoren schlussfolgern, dass Vertrauen in der Gesundheitsdatennutzung nicht global, sondern domänenspezifisch gemessen und politisch berücksichtigt werden muss.

Der Artikel „Data solidarity: Operationalising public value through a digital tool“ von Seliem El-Sayed, Ilona Kickbusch und Barbara Prainsack, veröffentlicht 2025 in der Zeitschrift Global Public Health, stellt das Konzept der Data Solidarity als Alternative zu herkömmlichen Datengovernance-Ansätzen vor. Diese basieren primär auf der Kategorisierung von Datentypen und vernachlässigen dabei den Kontext der Datennutzung sowie die Erzeugung öffentlichen Werts. Data Solidarity priorisiert stattdessen die Bewertung konkreter Datennutzungen: Nutzungen mit hohem öffentlichen Wert sollen aktiv gefördert, schädliche streng reguliert oder verboten und privat-profitorientierte ohne nennenswerten öffentlichen Nutzen durch Gewinnumverteilung (z. B. „Besteuerung“) an die Öffentlichkeit gebunden werden. Zur Operationalisierung dieses öffentlichen Werts wird das Bewertungstool PLUTO vorgestellt, das Risiken und Nutzen abwägt und eine gerechtere Verteilung von Vorteilen und Belastungen in der globalen Gesundheitsdatennutzung ermöglichen soll.

Der Artikel „New rules are required for WHO’s engagement with Big Tech“ (BMJ 2025;390:r1718), verfasst von Ilona Kickbusch und Kollegen, fordert die Entwicklung spezifischer Governance- und Accountability-Rahmenbedingungen für die Zusammenarbeit der Weltgesundheitsorganisation (WHO) mit großen Technologieunternehmen. Diese Unternehmen sind zu einflussreichen Akteuren in der globalen Gesundheit geworden, die Bereiche wie Gesundheitsdaten, KI-Anwendungen, Fehlinformationsbekämpfung und Politikgestaltung prägen. Der bestehende Framework of Engagement with Non-State Actors (FENSA) aus dem Jahr 2016 ist nicht ausreichend tech-spezifisch, weshalb die Autoren eine Anlehnung an die etablierten Regelungen für die Pharmaindustrie sowie die Einrichtung einer Arbeitsgruppe und langfristig eines Global Compact für digitale Gesundheit und KI-Governance vorschlagen, um Risiken wie Datenextraktion, algorithmische Bias und kommerzielle Interessenkonflikte zu minimieren und öffentliches Vertrauen zu wahren.

Die Veranstaltung „Trusting AI in Digital Health: A Transatlantic Dialogue on Ethics, Systems and Patients“ befasste sich mit der ethischen Gestaltung und dem Vertrauen in digitale Gesundheitssysteme. Dr. Girish Nadkarni, Chair der Abteilung für Künstliche Intelligenz und menschliche Gesundheit am Mount Sinai Health System, sowie Dr. Alexander Charney, Professor an der Icahn School of Medicine und Direktor des Charles Bronfman Institute for Personalized Medicine, forderten, Gesundheitsdaten nicht als bloße Handelsware, sondern als **öffentliches Gut** zu betrachten, wobei das Konzept von „Daten als Grundlage“ (soil) eine nachhaltige Kultivierung und Wiederverwendung zum gesellschaftlichen Nutzen vorsieht. Das Vertrauen in KI-Systeme muss dabei durch eine konsequente Governance, Transparenz und die Einhaltung strenger Datenschutzstandards wie HIPAA und DSGVO bereits in der Designphase verankert werden. Ein wesentlicher Aspekt ist zudem die Patientenzentrierung, da Patienten eher bereit sind, ihre Daten als öffentliche Ressource zur Verfügung zu stellen, wenn sie einen klaren altruistischen Nutzen für die medizinische Forschung und die Gesellschaft erkennen.

„Mein Doktor, die KI und ich“ ist eine Veranstaltungsreihe des Instituts für Ethik, Geschichte und Philosophie der Medizin der Medizinischen Hochschule Hannover. Das Projekt untersucht die Auswirkungen künstlicher Intelligenz auf die Beziehung zwischen Patient:innen und Ärzt:innen und entwickelt ethische Zukunftskonzepte für deren Umgang mit KI-Systemen in der Gesundheitsversorgung. Gefördert vom Niedersächsischen Ministerium für Wissenschaft und Kultur lief die Reihe von Oktober 2023 bis Dezember 2024. Im Mittelpunkt standen vier Diskursstationen mit Vorträgen, Workshops, illustrierten Fallvignetten sowie die Erarbeitung eines „living document“ mit konkreten Handlungsempfehlungen:

1. Verantwortung: Die Hauptverantwortung für den Einsatz von KI-Systemen sollten Ärzt:innen haben.
2. Vertrauen: Zertifizierungen, hohe wissenschaftliche Evidenz durch Studien und Kontrollmöglichkeiten sollen die entscheidenden Aspekte für die Vertrauenswürdigkeit darstellen. 
3. Rolle von Ärzt:innen: Ärzt:innen sollten eine beratende, unterstützende und empathische Rolle einnehmen, wenn KI beteiligt ist. 
4. KI verstehen: Patient:innen sollten von Ärzt:innen Informationen über die grundlegende Funktionsweise von KI in der Gesundheitsversorgung erhalten.
5. Konkreten KI-Einsatz hinterfragen: Patient:innen sollten von Ärzt:innen eine Erklärung für den konkreten Einsatz von KI bei ihrer Gesundheitsversorgung erhalten.
6. Datenschutz und Datensicherheit: Patient:innen sollten von Ärzt:innen eine Erläuterung über den Umgang mit ihren persönlichen Gesundheitsdaten beim Einsatz von KI erhalten.
7. Bedürfnisorientierung: Patient:innen sollten gemäß ihres individuellen Bedarfs gegenüber Ärzt:innen ihr Recht auf Wissen und auf Nicht-Wissen ausüben können.
8. Verantwortung und Haftung: Ärzt:innen sollten im Rahmen ihrer Sorgfaltspflicht die Verantwortung für KI-basierte Entscheidungen übernehmen.
9. Transparenz und Erklärbarkeit: Ärzt:innen sollten die Funktionsweise von KI-Systemen verstehen und bei Bedarf eine Erklärung für Ergebnisse geben können.
10. Patientenkommunikation und informierte Einwilligung: Ärzt:innen sollten über die Funktionsweise, Chancen, Risiken und Grenzen des Einsatzes von KI aufklären können.
11. Weiterbildung: Ärzt:innen sollten die Möglichkeit haben, sich über die neuesten Entwicklungen im Bereich KI fortzubilden.
