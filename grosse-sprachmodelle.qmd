# Große Sprachmodelle 

## Klinische Sicherheit und Halluzinationsraten

„A Framework to Assess Clinical Safety and Hallucination Rates of LLMs for Medical Text Summarisation“ stellt ein Framework vor, das die klinische Sicherheit von Large Language Models bei der Zusammenfassung medizinischer Texte bewertet. Im Fokus der Schadensdefinition und -einteilung stehen Fehler wie Halluzinationen und Omissions, die in major und minor kategorisiert werden: Major-Fehler wirken sich auf die Diagnose oder das Management des Patienten aus und bergen potenzielles Harm, während minor-Fehler keinen relevanten Einfluss auf die Patientensicherheit haben. Die Schadenseinteilung orientiert sich an Protokollen für medizinische Geräte und kombiniert die Wahrscheinlichkeit eines Fehlers mit seiner Konsequenz (z. B. Anzahl betroffener Patienten und Schweregrad), um ein Risiko-Level zu ermitteln. Dies ermöglicht eine risikobasierte Bewertung und Priorisierung kritischer Fehler in der klinischen Anwendung. In der untersuchten Studie betrug die Häufigkeit von Halluzinationen in den generierten klinischen Notizen 1,47 % der Sätze, wobei 44 % dieser Halluzinationen als major eingestuft wurden und potenziell die Diagnose oder das Patientenmanagement beeinträchtigen könnten. Im Vergleich dazu traten Omissions-Fehler häufiger auf, mit einer Rate von 3,45 % der relevanten Sätze aus den Transkripten, von denen etwa 16,7 % major waren und klinisch bedeutsame Informationen ausließen. Durch iterative Optimierungen von Prompts und Workflows konnten in den besten Experimenten die Raten majorer Halluzinationen und Omissions unter die in der Literatur berichteten Werte für menschlich erstellte Notizen gesenkt werden, die durchschnittlich mindestens einen Fehler und vier Omissions pro Notiz aufweisen. Dies unterstreicht, dass Omissions-Fehler in der klinischen Textzusammenfassung tendenziell häufiger vorkommen als Halluzinationen, letztere jedoch ein höheres Risiko für schwere klinische Konsequenzen bergen.

## Agentische KI

MedAgentBench v2 Improving Medical LLM Agent Design untersucht, wie sich das Design eines klinischen KI‑Agents in einer FHIR‑konformen elektronischen Patientenakte durch gezieltes Prompt‑Engineering, spezialisierte Werkzeuge und eine Memory‑Komponente verbessern lässt. Der Beitrag beschreibt, wie neue Tools für strukturierte FHIR‑Interaktionen, Rechenoperationen und formatierte Ausgabe zusammen mit einem überarbeiteten Systemprompt die Erfolgsrate des auf GPT‑4.1 basierenden Agents auf 91 % ohne und 98 % mit Memory steigern. Zudem werden 300 zusätzliche, mehrschrittige klinische Aufgaben entwickelt, um Generalisierbarkeit und Grenzen des Ansatzes zu evaluieren und Anforderungen für eine verantwortliche Einführung agentischer KI in realen Versorgungsszenarien zu skizzieren.

## Weiteres

Die Studie „Evaluating the performance of general purpose large language models in identifying human facial emotions“ untersuchte die Fähigkeit dreier führender Large Language Models (GPT-4o, Gemini 2.0 Experimental und Claude 3.5 Sonnet), menschliche Gesichtsausdrücke anhand des NimStim-Datensatzes korrekt zu erkennen. GPT-4o und Gemini 2.0 Experimental erreichten eine nahezu perfekte Übereinstimmung mit den Ground-Truth-Labels (Cohen’s Kappa 0,83 bzw. 0,81) und lagen insgesamt im Bereich oder teilweise über der Leistung menschlicher Beurteiler. Alle Modelle zeigten insbesondere bei den Kategorien calm/neutral, happy und surprise gute Ergebnisse, wiesen jedoch deutliche Schwierigkeiten bei der Erkennung von fear auf, das häufig als surprise fehlklassifiziert wurde. Claude 3.5 Sonnet erreichte mit Kappa 0,70 und 74 % Gesamtgenauigkeit eine deutlich geringere Übereinstimmung. Die Leistung der Modelle variierte weder systematisch nach Geschlecht noch nach Ethnie der dargestellten Personen.

Die Studie „Reliability of LLMs as medical assistants for the general public: a randomized preregistered study“, veröffentlicht am 9. Februar 2026 in Nature Medicine, untersucht die Zuverlässigkeit großer Sprachmodelle (LLMs) wie GPT-4o, Llama 3 und Command R+ als medizinische Berater für Laien. In einem randomisierten kontrollierten Versuch mit 1.298 Teilnehmern aus dem Vereinigten Königreich bearbeiteten Probanden zehn realistische medizinische Szenarien, um mögliche Erkrankungen zu erkennen und eine angemessene Handlungsempfehlung (Disposition) zu wählen. Während die Modelle allein in 94,9 % der Fälle relevante Erkrankungen korrekt identifizierten und in 56,3 % die richtige Disposition empfahlen, erreichten Teilnehmer mit LLM-Unterstützung deutlich schlechtere Werte (unter 34,5 % bei Erkrankungen und unter 44,2 % bei Disposition), die nicht besser als in der Kontrollgruppe ohne KI waren. Die Autoren führen dies auf Interaktionsprobleme zwischen Nutzern und Modellen zurück und betonen, dass gängige Benchmarks sowie Simulationen die tatsächlichen Schwächen in realen Mensch-KI-Interaktionen nicht vorhersagen. Sie empfehlen daher systematische Tests mit echten Nutzern vor einem breiten Einsatz von LLMs in der öffentlichen Gesundheitsberatung.

Die Studie „Mapping the susceptibility of large language models to medical misinformation across clinical notes and social media: a cross-sectional benchmarking analysis“ wurde im Januar 2026 in der Zeitschrift The Lancet Digital Health (Volume 8, Issue 1, Artikel 100949) veröffentlicht. Sie untersucht in einer umfangreichen Querschnittsanalyse die Anfälligkeit von 20 großen Sprachmodellen (LLMs) für medizinische Fehlinformationen anhand von über 3,4 Millionen Prompts. Diese stammten aus drei Quellen: realen Krankenhaus-Entlassungsberichten mit eingefügten falschen Empfehlungen, echten Social-Media-Beiträgen (Reddit) mit medizinischen Mythen sowie validierten simulierten klinischen Szenarien. Zusätzlich wurde getestet, wie die Umformulierung der Inhalte als logische Fehlschlüsse (z. B. Appeal to Popularity oder Slippery Slope) die Akzeptanzrate beeinflusst. Die Ergebnisse zeigen eine durchschnittliche Anfälligkeit von etwa 32 % bei neutral formulierten falschen Aussagen, mit deutlich höheren Werten (46 %) bei klinisch formulierten Texten und niedrigeren bei informellen Social-Media-Inhalten; die meisten Fehlschluss-Umformulierungen verringerten die Anfälligkeit signifikant, während einige (wie Appeal to Authority) sie leicht erhöhten. Die Autoren schließen, dass die Sicherheit von LLMs im medizinischen Bereich stärker von kontextbezogenen Schutzmechanismen und Faktenverankerung abhängt als von Modellgröße allein.

Die Studie mit dem Titel „Mapping the susceptibility of large language models to medical misinformation across clinical notes and social media: a cross-sectional benchmarking analysis“ erschien im Januar 2026 in The Lancet Digital Health. In dieser großangelegten Querschnittsanalyse untersuchten die Autoren die Anfälligkeit von 20 Large Language Models (LLMs) für medizinische Fehlinformationen anhand von über 3,4 Millionen Prompts. Diese stammten aus drei Quellen: realen Krankenhaus-Entlassungsberichten (mit eingefügter falscher Empfehlung), Social-Media-Beiträgen (z. B. Reddit) und validierten simulierten klinischen Vignetten. Die Modelle zeigten insgesamt in 31,7 % der neutralen Basis-Prompts Anfälligkeit für die fabrizierten Inhalte, wobei die höchste Rate (46,1 %) bei modifizierten klinischen Notizen auftrat und die niedrigste (8,9 %) bei Social-Media-Inhalten. Die Umformulierung der falschen Aussagen als logische Fehlschlüsse (z. B. Appeal to Popularity) reduzierte die Akzeptanzrate in den meisten Fällen signifikant, während Appeal to Authority und Slippery Slope sie teilweise erhöhten. GPT-Modelle erwiesen sich als am wenigsten anfällig und am besten in der Erkennung von Fehlschlüssen, während die Ergebnisse die Notwendigkeit besserer faktengestützter Schutzmechanismen in medizinischen Anwendungen unterstreichen.

Die Studie „Reliability of LLMs as medical assistants for the general public: a randomized preregistered study“ (veröffentlicht am 9. Februar 2026 in Nature Medicine) untersucht, ob große Sprachmodelle (LLMs) wie GPT-4o, Llama 3 und Command R+ die Allgemeinbevölkerung zuverlässig bei medizinischen Entscheidungen unterstützen können. In einer randomisierten kontrollierten Studie mit 1.298 Teilnehmern aus dem Vereinigten Königreich bearbeiteten Probanden zehn realistische medizinische Szenarien, entweder mit Unterstützung eines LLM oder mit selbst gewählten Hilfsmitteln (meist Internetsuche, Kontrollgruppe). Allein eingesetzt erreichten die Modelle eine hohe Genauigkeit (durchschnittlich 94,9 % bei der Identifikation relevanter Erkrankungen und 56,3 % bei der richtigen Handlungsempfehlung). Bei der Nutzung durch Laien sanken diese Werte jedoch deutlich (unter 34,5 % für Erkrankungen und unter 44,2 % für Dispositionen) und lagen nicht über denen der Kontrollgruppe. Die Autoren führen dies auf Schwächen in der Mensch-KI-Interaktion zurück, wie unvollständige Angaben der Nutzer, Fehlinterpretationen durch die Modelle sowie mangelnde Übernahme korrekter Vorschläge. Standard-Benchmarks und Simulationen prognostizieren diese Probleme nicht zuverlässig; die Studie empfiehlt daher systematische Tests mit realen Nutzern vor einem breiten Einsatz von LLMs als medizinische Berater für die Öffentlichkeit.

„Holistic evaluation of large language models for medical tasks with MedHELM“ ist ein in Nature Medicine am 20. Januar 2026 veröffentlichter Artikel von Suhana Bedi, Hejie Cui und weiteren Autoren, darunter Nigam H. Shah. Die Arbeit stellt das MedHELM-Framework vor, eine erweiterbare Evaluationsmethode für Large Language Models (LLMs) in medizinischen Anwendungen. Es umfasst eine klinisch validierte Taxonomie mit fünf Hauptkategorien (Clinical Decision Support, Clinical Note Generation, Patient Communication, Medical Research und Administration), 22 Unterkategorien und 121 konkreten Aufgaben, die reale klinische Praxis abbilden. Ergänzt wird dies durch 37 Benchmarks und einen systematischen Vergleich neun führender LLMs mittels einer automatisierten LLM-Jury-Bewertung. Die Ergebnisse zeigen, dass fortgeschrittene Reasoning-Modelle wie DeepSeek R1 und o3-mini die höchsten Win-Rates erreichen, während Modelle wie Claude 3.5 Sonnet vergleichbare Leistungen bei geringerem Rechenaufwand erbringen. MedHELM ermöglicht damit eine evidenzbasierte Auswahl von KI-Systemen für den Gesundheitsbereich und ist als Open-Source-Code verfügbar.

Die Studie „Performance of Large Language Models Under Input Variability in Health Care Applications: Dataset Development and Experimental Evaluation“ von Saubhagya Joshi und Kollegen wurde am 20. Februar 2026 in JMIR AI (Band 5, e83640) veröffentlicht. Sie untersucht die Robustheit von drei Large Language Models gegenüber variierenden Eingaben im Gesundheitsbereich, indem sie einen neu entwickelten Datensatz mit drei Arten menschlicher Störungen (Redaktionen, Homophone und typografische Fehler) in unterschiedlichen Intensitätsstufen nutzt und diese auf drei gesundheitsbezogene Aufgaben anwendet. Entgegen den Erwartungen zeigten die Modelle eine bemerkenswerte Widerstandsfähigkeit: In über der Hälfte der Fälle (55,92 %) blieb die Leistung stabil oder verbesserte sich sogar, insbesondere bei geringen Störungsgraden, während Redaktionen (z. B. durch Datenschutz oder kognitive Einschränkungen) den stärksten negativen Einfluss hatten. Die Ergebnisse unterstreichen die Notwendigkeit, LLM-basierte Anwendungen im Gesundheitswesen gezielt auf reale Eingabevariabilität auszulegen, und stellen den begleitenden Datensatz als Ressource für weitere Robustheitsforschung zur Verfügung.
