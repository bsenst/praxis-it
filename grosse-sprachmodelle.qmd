# Große Sprachmodelle 

## Klinische Sicherheit und Halluzinationsraten

„A Framework to Assess Clinical Safety and Hallucination Rates of LLMs for Medical Text Summarisation“ stellt ein Framework vor, das die klinische Sicherheit von Large Language Models bei der Zusammenfassung medizinischer Texte bewertet. Im Fokus der Schadensdefinition und -einteilung stehen Fehler wie Halluzinationen und Omissions, die in major und minor kategorisiert werden: Major-Fehler wirken sich auf die Diagnose oder das Management des Patienten aus und bergen potenzielles Harm, während minor-Fehler keinen relevanten Einfluss auf die Patientensicherheit haben. Die Schadenseinteilung orientiert sich an Protokollen für medizinische Geräte und kombiniert die Wahrscheinlichkeit eines Fehlers mit seiner Konsequenz (z. B. Anzahl betroffener Patienten und Schweregrad), um ein Risiko-Level zu ermitteln. Dies ermöglicht eine risikobasierte Bewertung und Priorisierung kritischer Fehler in der klinischen Anwendung. In der untersuchten Studie betrug die Häufigkeit von Halluzinationen in den generierten klinischen Notizen 1,47 % der Sätze, wobei 44 % dieser Halluzinationen als major eingestuft wurden und potenziell die Diagnose oder das Patientenmanagement beeinträchtigen könnten. Im Vergleich dazu traten Omissions-Fehler häufiger auf, mit einer Rate von 3,45 % der relevanten Sätze aus den Transkripten, von denen etwa 16,7 % major waren und klinisch bedeutsame Informationen ausließen. Durch iterative Optimierungen von Prompts und Workflows konnten in den besten Experimenten die Raten majorer Halluzinationen und Omissions unter die in der Literatur berichteten Werte für menschlich erstellte Notizen gesenkt werden, die durchschnittlich mindestens einen Fehler und vier Omissions pro Notiz aufweisen. Dies unterstreicht, dass Omissions-Fehler in der klinischen Textzusammenfassung tendenziell häufiger vorkommen als Halluzinationen, letztere jedoch ein höheres Risiko für schwere klinische Konsequenzen bergen.

