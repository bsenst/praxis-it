# Große Sprachmodelle 

## Klinische Sicherheit und Halluzinationsraten

„A Framework to Assess Clinical Safety and Hallucination Rates of LLMs for Medical Text Summarisation“ stellt ein Framework vor, das die klinische Sicherheit von Large Language Models bei der Zusammenfassung medizinischer Texte bewertet. Im Fokus der Schadensdefinition und -einteilung stehen Fehler wie Halluzinationen und Omissions, die in major und minor kategorisiert werden: Major-Fehler wirken sich auf die Diagnose oder das Management des Patienten aus und bergen potenzielles Harm, während minor-Fehler keinen relevanten Einfluss auf die Patientensicherheit haben. Die Schadenseinteilung orientiert sich an Protokollen für medizinische Geräte und kombiniert die Wahrscheinlichkeit eines Fehlers mit seiner Konsequenz (z. B. Anzahl betroffener Patienten und Schweregrad), um ein Risiko-Level zu ermitteln. Dies ermöglicht eine risikobasierte Bewertung und Priorisierung kritischer Fehler in der klinischen Anwendung. In der untersuchten Studie betrug die Häufigkeit von Halluzinationen in den generierten klinischen Notizen 1,47 % der Sätze, wobei 44 % dieser Halluzinationen als major eingestuft wurden und potenziell die Diagnose oder das Patientenmanagement beeinträchtigen könnten. Im Vergleich dazu traten Omissions-Fehler häufiger auf, mit einer Rate von 3,45 % der relevanten Sätze aus den Transkripten, von denen etwa 16,7 % major waren und klinisch bedeutsame Informationen ausließen. Durch iterative Optimierungen von Prompts und Workflows konnten in den besten Experimenten die Raten majorer Halluzinationen und Omissions unter die in der Literatur berichteten Werte für menschlich erstellte Notizen gesenkt werden, die durchschnittlich mindestens einen Fehler und vier Omissions pro Notiz aufweisen. Dies unterstreicht, dass Omissions-Fehler in der klinischen Textzusammenfassung tendenziell häufiger vorkommen als Halluzinationen, letztere jedoch ein höheres Risiko für schwere klinische Konsequenzen bergen.

## Agentische KI

MedAgentBench v2 Improving Medical LLM Agent Design untersucht, wie sich das Design eines klinischen KI‑Agents in einer FHIR‑konformen elektronischen Patientenakte durch gezieltes Prompt‑Engineering, spezialisierte Werkzeuge und eine Memory‑Komponente verbessern lässt. Der Beitrag beschreibt, wie neue Tools für strukturierte FHIR‑Interaktionen, Rechenoperationen und formatierte Ausgabe zusammen mit einem überarbeiteten Systemprompt die Erfolgsrate des auf GPT‑4.1 basierenden Agents auf 91 % ohne und 98 % mit Memory steigern. Zudem werden 300 zusätzliche, mehrschrittige klinische Aufgaben entwickelt, um Generalisierbarkeit und Grenzen des Ansatzes zu evaluieren und Anforderungen für eine verantwortliche Einführung agentischer KI in realen Versorgungsszenarien zu skizzieren.

## Weiteres

Die Studie „Evaluating the performance of general purpose large language models in identifying human facial emotions“ untersuchte die Fähigkeit dreier führender Large Language Models (GPT-4o, Gemini 2.0 Experimental und Claude 3.5 Sonnet), menschliche Gesichtsausdrücke anhand des NimStim-Datensatzes korrekt zu erkennen. GPT-4o und Gemini 2.0 Experimental erreichten eine nahezu perfekte Übereinstimmung mit den Ground-Truth-Labels (Cohen’s Kappa 0,83 bzw. 0,81) und lagen insgesamt im Bereich oder teilweise über der Leistung menschlicher Beurteiler. Alle Modelle zeigten insbesondere bei den Kategorien calm/neutral, happy und surprise gute Ergebnisse, wiesen jedoch deutliche Schwierigkeiten bei der Erkennung von fear auf, das häufig als surprise fehlklassifiziert wurde. Claude 3.5 Sonnet erreichte mit Kappa 0,70 und 74 % Gesamtgenauigkeit eine deutlich geringere Übereinstimmung. Die Leistung der Modelle variierte weder systematisch nach Geschlecht noch nach Ethnie der dargestellten Personen.

Die Studie „Reliability of LLMs as medical assistants for the general public: a randomized preregistered study“, veröffentlicht am 9. Februar 2026 in Nature Medicine, untersucht die Zuverlässigkeit großer Sprachmodelle (LLMs) wie GPT-4o, Llama 3 und Command R+ als medizinische Berater für Laien. In einem randomisierten kontrollierten Versuch mit 1.298 Teilnehmern aus dem Vereinigten Königreich bearbeiteten Probanden zehn realistische medizinische Szenarien, um mögliche Erkrankungen zu erkennen und eine angemessene Handlungsempfehlung (Disposition) zu wählen. Während die Modelle allein in 94,9 % der Fälle relevante Erkrankungen korrekt identifizierten und in 56,3 % die richtige Disposition empfahlen, erreichten Teilnehmer mit LLM-Unterstützung deutlich schlechtere Werte (unter 34,5 % bei Erkrankungen und unter 44,2 % bei Disposition), die nicht besser als in der Kontrollgruppe ohne KI waren. Die Autoren führen dies auf Interaktionsprobleme zwischen Nutzern und Modellen zurück und betonen, dass gängige Benchmarks sowie Simulationen die tatsächlichen Schwächen in realen Mensch-KI-Interaktionen nicht vorhersagen. Sie empfehlen daher systematische Tests mit echten Nutzern vor einem breiten Einsatz von LLMs in der öffentlichen Gesundheitsberatung.
